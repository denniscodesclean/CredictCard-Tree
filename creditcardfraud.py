# -*- coding: utf-8 -*-
"""CreditCardFraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1smkWWxMIujUqZQS3XDfCnw6Vlr3-YCDz
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import lightgbm as lgb
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import classification_report,accuracy_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost import XGBClassifier
import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import IsolationForest
import os
import random
def SeedEverything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED']=str(seed)
    np.random.seed(seed)
    return
SeedEverything(123)

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CreditCardFraud/creditcard.csv')
data.head()

data.shape

# Examine balance of dataset
print(data['Class'].value_counts())
print("Fraud Ratio in Data: {:.6f}".format(data['Class'].value_counts()[1]/data['Class'].value_counts()[0]))

# Train Test Split
X = data.drop(['Class', 'Time'], axis=1)
y = data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train) # avoid data leakage
X_test = scaler.transform(X_test)

# KFolds
n_folds = 5
folds = StratifiedKFold(n_splits=n_folds, shuffle=True,random_state=123)
splits = folds.split(X, y)

# Logistic Regression
y_oof = np.zeros(y.shape[0]) # out-of-fold
for fold_n, (train_index, valid_index) in enumerate(splits):
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

    lr = LogisticRegression(n_jobs=-1, max_iter=200) # Default max_iter cannot converge
    lr.fit(X_train, y_train)

    y_pred_valid = lr.predict_proba(X_valid)[:,1] # probability predicted for Class 1
    y_oof[valid_index] = y_pred_valid

    print(f"Fold: {fold_n}, AUC: {roc_auc_score(y_valid, y_pred_valid)}")

print(f"OOF AUC: {roc_auc_score(y, y_oof)}")

'''
Fold: 0, AUC: 0.9884841059594415
Fold: 1, AUC: 0.9472227862217839
Fold: 2, AUC: 0.9535611370975065
Fold: 3, AUC: 0.9737342204876956
Fold: 4, AUC: 0.9637883678170984
OOF AUC: 0.9640431880990812
'''

# KFolds
n_folds = 5
folds = StratifiedKFold(n_splits=n_folds, shuffle=True,random_state=123)
splits = folds.split(X, y)

# Light Gradient Boost (LGB)
# Define parameters for LightGBM
params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 400,
    'max_depth': -1,
    'learning_rate': 0.006,
    'feature_fraction': 0.4,
    'min_data_in_leaf': 100,
    'verbosity': -1,
    "bagging_seed": 123,
    'random_state': 123,
}

y_oof = np.zeros(y.shape[0]) # out-of-fold
for fold_n, (train_index, valid_index) in enumerate(splits):
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

    lgb_train = lgb.Dataset(X_train, y_train)
    lgb_valid = lgb.Dataset(X_valid, y_valid)

    clf = lgb.train(params, lgb_train, 10000, valid_sets = [lgb_train, lgb_valid],callbacks=[lgb.early_stopping(stopping_rounds=100)])

    y_pred_valid = clf.predict(X_valid)
    y_oof[valid_index] = y_pred_valid
    print(f"Fold: {fold_n}, AUC: {roc_auc_score(y_valid, y_pred_valid)}")

print(f"OOF AUC: {roc_auc_score(y, y_oof)}")

'''

'''

# KFolds
n_folds = 5
folds = StratifiedKFold(n_splits=n_folds, shuffle=True,random_state=123)

# Random Forest
rf = RandomForestClassifier(oob_score=True, random_state=123)

# Define the hyperparameters grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2'],
    'bootstrap': [True]
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=5,  # Number of random combinations to try
    cv=folds,
    verbose=2,
    random_state=123,
    n_jobs=-1
)

random_search.fit(X_train, y_train)

# Print the best hyperparameters
print(f"Best Hyperparameters: {random_search.best_params_}")
best_rf = random_search.best_estimator_

oob_score = best_rf.oob_score_
print(f"Out-of-Bag Accuracy: {oob_score}")